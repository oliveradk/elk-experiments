{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import Image, display\n",
    "os.chdir(\"/Users/oliverdaniels-koch/projects/elk-experiments\")\n",
    "out_dir = Path(\"output\")\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformer_lens import HookedTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug set_use_plit_qkv_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outputs are inconistent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"pythia-70m\")\n",
    "# show inconsistency \n",
    "test_str = \"Hello, world\"\n",
    "out_normal = model(test_str)\n",
    "\n",
    "# set up hooks\n",
    "model.set_use_split_qkv_input(True)\n",
    "out_split = model(test_str)\n",
    "\n",
    "torch.allclose(out_normal, out_split, atol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0015, device='mps:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(out_normal - out_split).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outupts are inconsistent across batch sizes\n",
    "\n",
    "seems to work on imdb, but not my other data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# load subset of dataset and tokenize\n",
    "model_name = \"pythia-70m\"\n",
    "dataset = datasets.load_dataset(\"imdb\", split=\"train[:10]\")\n",
    "tokens = model.tokenizer(dataset[\"text\"], padding=True, truncation=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "model = HookedTransformer.from_pretrained(model_name)\n",
    "model.set_use_split_qkv_input(True)\n",
    "out_batch = model(tokens[:4])\n",
    "out_single = model(tokens[:1])\n",
    "assert torch.allclose(out_batch[0], out_single[0], atol=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hook\n",
    "model.set_use_split_qkv_input(True)\n",
    "out_batch = model(tokens[:4])\n",
    "out_single = model(tokens[:1])\n",
    "assert torch.allclose(out_batch[0], out_single[0], atol=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='mps:0', grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(out_batch[0] - out_single[0]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model attn-only-1l into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n",
      "Moving model to device:  mps\n"
     ]
    }
   ],
   "source": [
    "# load task\n",
    "from cupbearer import tasks\n",
    "task = tasks.tiny_natural_mechanisms(\"hex\", \"mps\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trusted_tokens = torch.stack([torch.tensor(data[\"prefix_tokens\"]) for data in task.trusted_data.data])\n",
    "untrusted_clean_tokens = torch.stack([torch.tensor(data[\"prefix_tokens\"]) for data in task.test_data.normal_data.data])\n",
    "anomalous_tokens = torch.stack([torch.tensor(data[\"prefix_tokens\"]) for data in task.test_data.anomalous_data.data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(model_name)\n",
    "out_batch = model(trusted_tokens[:4])\n",
    "out_single = model(trusted_tokens[:1])\n",
    "assert torch.allclose(out_batch[0], out_single[0], atol=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"pythia-70m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "tensor(0.0052, device='mps:0', grad_fn=<MaxBackward1>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m out_batch \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m[:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      9\u001b[0m out_single \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m[:\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(out_batch[\u001b[38;5;241m0\u001b[39m], out_single[\u001b[38;5;241m0\u001b[39m], atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m), torch\u001b[38;5;241m.\u001b[39mabs(out_batch[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m out_single[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor(0.0052, device='mps:0', grad_fn=<MaxBackward1>)"
     ]
    }
   ],
   "source": [
    "# set hook\n",
    "input = torch.tensor([\n",
    "    [2, 2, 2, 69, 26, 67, 17, 14, 14836, 3593, 14, 21, 12347, 14, 1257, 24],\n",
    "    [535, 50270, 338, 1881, 15, 2364, 15, 25950, 2073, 15741, 64, 29786,  3401, 35495, 686, 26]\n",
    "])\n",
    "model_name = \"pythia-70m\"\n",
    "model = HookedTransformer.from_pretrained(model_name)\n",
    "model.set_use_split_qkv_input(True)\n",
    "out_batch = model(input[:2])\n",
    "out_single = model(input[:1])\n",
    "assert torch.allclose(out_batch[0], out_single[0], atol=1e-3), torch.abs(out_batch[0] - out_single[0]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add padding token to all tokens \n",
    "trusted_tokens_padded = torch.cat([trusted_tokens, torch.zeros_like(trusted_tokens[:, :1])], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "tensor(0.0052, device='mps:0', grad_fn=<MaxBackward1>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m out_batch \u001b[38;5;241m=\u001b[39m model(trusted_tokens_padded[:\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m      4\u001b[0m out_single \u001b[38;5;241m=\u001b[39m model(trusted_tokens_padded[:\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mallclose(out_batch[\u001b[38;5;241m0\u001b[39m], out_single[\u001b[38;5;241m0\u001b[39m], atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m), torch\u001b[38;5;241m.\u001b[39mabs(out_batch[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m out_single[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mmax()\n",
      "\u001b[0;31mAssertionError\u001b[0m: tensor(0.0052, device='mps:0', grad_fn=<MaxBackward1>)"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(model_name)\n",
    "model.set_use_split_qkv_input(True)\n",
    "out_batch = model(trusted_tokens_padded[:4])\n",
    "out_single = model(trusted_tokens_padded[:1])\n",
    "assert torch.allclose(out_batch[0], out_single[0], atol=5e-4), torch.abs(out_batch[0] - out_single[0]).max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
