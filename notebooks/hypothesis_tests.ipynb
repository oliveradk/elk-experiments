{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda visible devices\n",
    "from elk_experiments.utils import is_notebook\n",
    "import os\n",
    "if is_notebook():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" #\"1\"\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "    os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing Automatically Discovered Circuits\n",
    "\n",
    "Procedure: \n",
    "- Compute prune scores (via attribution patching) \n",
    "- Search over different thresholds to find the smallest circuit where the null hypotheis of Equivalence / Dominance cannot be rejected \n",
    "- Prune edges from circuit that are not in paths to the output, or in the case of resample ablation cannot be reached from the input\n",
    "- Test whether each edge in the circuit is minimal \n",
    "- Test whether the circuit is complete (by seeing if the null hypothesis on the independence test can be rejected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Minimal Faithful Circuit According to Prune Score Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Callable, Dict, Tuple, Union, Optional, Any, Literal, NamedTuple\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import binom, beta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from auto_circuit.types import (\n",
    "    CircuitOutputs, \n",
    "    BatchKey,\n",
    "    PruneScores,\n",
    "    PatchType, \n",
    "    AblationType,\n",
    "    SrcNode, \n",
    "    DestNode, \n",
    "    Edge,\n",
    "    Node\n",
    ")\n",
    "from auto_circuit.data import PromptPairBatch, PromptDataLoader   \n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.visualize import draw_seq_graph\n",
    "from auto_circuit.utils.tensor_ops import batch_answer_diffs, batch_avg_answer_val\n",
    "from auto_circuit.utils.patchable_model import PatchableModel\n",
    "from auto_circuit.utils.custom_tqdm import tqdm\n",
    "\n",
    "from elk_experiments.auto_circuit.auto_circuit_utils import (\n",
    "    run_circuits,\n",
    "    desc_prune_scores, \n",
    "    prune_scores_threshold, \n",
    "    load_tf_model\n",
    ")\n",
    "from elk_experiments.auto_circuit.score_funcs import GradFunc, AnswerFunc, get_score_func\n",
    "\n",
    "from elk_experiments.auto_circuit.circuit_hypotests import (\n",
    "    get_edge_idx, \n",
    "    edges_from_mask,\n",
    "    equiv_test,\n",
    "    sweep_search_smallest_equiv,\n",
    "    plot_num_ablated_C_gt_M, \n",
    "    plot_circuit_and_model_scores,\n",
    "    compute_knees, \n",
    "    plot_edge_scores_and_knees,\n",
    "    minimality_test, \n",
    "    plot_p_values, \n",
    "    plot_edge_k, \n",
    "    plot_score_quantiles,\n",
    "    independence_test, \n",
    "    result_to_json\n",
    ")\n",
    "\n",
    "from elk_experiments.auto_circuit.node_graph import (\n",
    "    NodeGraph, \n",
    "    SeqNode, \n",
    "    NodeIdx,\n",
    "    SampleType,\n",
    "    get_node_idx, \n",
    "    sample_paths, \n",
    "    visualize_graph, \n",
    "    edge_in_path\n",
    ")\n",
    "from elk_experiments.auto_circuit.tasks import TASK_DICT\n",
    "from elk_experiments.utils import OUTPUT_DIR, repo_path_to_abs_path, load_cache, save_cache, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config class\n",
    "@dataclass \n",
    "class Config: \n",
    "    task: str = \"Docstring Token Circuit\"\n",
    "    use_abs: bool = False\n",
    "    ablation_type: Union[AblationType, str] = AblationType.TOKENWISE_MEAN_CORRUPT \n",
    "    grad_func: Optional[Union[GradFunc, str]] = None\n",
    "    answer_func: Optional[Union[AnswerFunc, str]] = None\n",
    "    ig_samples: int = 10, \n",
    "    alpha: float = 0.05\n",
    "    epsilon: Optional[float] = None\n",
    "    q_star: float = 0.9 \n",
    "    grad_func_mask: Optional[Union[GradFunc, str]] = None\n",
    "    answer_func_mask: Optional[Union[AnswerFunc, str]] = None\n",
    "    clean_corrupt: Optional[Literal[\"clean\", \"corrupt\"]] = None\n",
    "    side: Optional[Literal[\"left\", \"right\", \"none\"]] = None\n",
    "    save_cache: bool = True\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if isinstance(self.ablation_type, str):\n",
    "            self.ablation_type = AblationType[self.ablation_type.upper()]\n",
    "        if isinstance(self.grad_func, str):\n",
    "            self.grad_func = GradFunc[self.grad_func.upper()]\n",
    "        elif self.grad_func is None:\n",
    "            self.grad_func = GradFunc.LOGPROB if not self.use_abs else GradFunc.LOGIT\n",
    "        if isinstance(self.answer_func, str):\n",
    "            self.answer_func = AnswerFunc[self.answer_func.upper()]\n",
    "        elif self.answer_func is None:\n",
    "            self.answer_func = AnswerFunc.AVG_VAL if not self.use_abs else AnswerFunc.AVG_DIFF\n",
    "        if self.epsilon is None:\n",
    "            self.epsilon = 0.1 if self.use_abs else 0.0\n",
    "        if isinstance(self.grad_func_mask, str):\n",
    "            self.grad_func_mask = GradFunc[self.grad_func_mask.upper()]\n",
    "        elif self.grad_func_mask is None:\n",
    "            self.grad_func_mask = self.grad_func\n",
    "        if isinstance(self.answer_func_mask, str):\n",
    "            self.answer_func_mask = AnswerFunc[self.answer_func_mask.upper()]\n",
    "        elif self.answer_func_mask is None:\n",
    "            self.answer_func_mask = self.answer_func\n",
    "        if self.clean_corrupt is None:\n",
    "            self.clean_corrupt = \"corrupt\" if self.ablation_type == AblationType.RESAMPLE else None\n",
    "        if self.side is None:\n",
    "            self.side = None if self.use_abs else \"left\" \n",
    "        elif self.side == \"none\":\n",
    "            self.side = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize config \n",
    "conf = Config()\n",
    "# get config overrides if runnign from command line\n",
    "if not is_notebook():\n",
    "    import sys \n",
    "    conf = OmegaConf.merge(OmegaConf.structured(conf), OmegaConf.from_cli(sys.argv[1:]))\n",
    "\n",
    "# handle directories\n",
    "out_dir = OUTPUT_DIR\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "score_dir = out_dir / f\"{conf.task.replace(' ', '_')}_{conf.ablation_type.name}_{conf.grad_func.name}_{conf.answer_func.name}_{conf.ig_samples}\" \n",
    "score_dir.mkdir(exist_ok=True)\n",
    "exp_dir = score_dir / f\"{conf.use_abs}_{conf.alpha}_{conf.epsilon}_{conf.q_star}\"\n",
    "exp_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize task\n",
    "task = TASK_DICT[conf.task]\n",
    "task.init_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute edge scores\n",
    "# TODO: pass full model\n",
    "prune_scores = mask_gradient_prune_scores(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader,\n",
    "    official_edges=None,\n",
    "    grad_function=conf.grad_func_mask.value, \n",
    "    answer_function=conf.answer_func_mask.value, #answer_function,\n",
    "    mask_val=None, \n",
    "    ablation_type=conf.ablation_type,\n",
    "    integrated_grad_samples=10, # 10 1 for debugging\n",
    "    clean_corrupt=conf.clean_corrupt,\n",
    ")\n",
    "if conf.save_cache:\n",
    "    save_cache(prune_scores, exp_dir, \"prune_scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_out_train: dict[BatchKey, torch.Tensor] = {\n",
    "    batch.key: task.model(batch.clean)[task.model.out_slice] \n",
    "    for batch in task.train_loader\n",
    "}\n",
    "model_out_test: dict[BatchKey, torch.Tensor] = {\n",
    "    batch.key: task.model(batch.clean)[task.model.out_slice] \n",
    "    for batch in task.test_loader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_results, min_equiv = sweep_search_smallest_equiv(\n",
    "    model=task.model, \n",
    "    dataloader=task.train_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    grad_function=conf.grad_func, \n",
    "    answer_function=conf.answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    use_abs=conf.use_abs,\n",
    "    side=conf.side,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.side,\n",
    "    model_out=model_out_train,\n",
    ")\n",
    "save_json(result_to_json(equiv_results), exp_dir, \"equiv_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equiv_test_result = equiv_test(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=prune_scores,\n",
    "    grad_func=conf.grad_func, \n",
    "    answer_function=conf.answer_func,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    edge_counts=[min_equiv],\n",
    "    use_abs=conf.use_abs,\n",
    "    side=conf.side,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.epsilon,\n",
    "    model_out=model_out_test\n",
    ")\n",
    "save_json(result_to_json(equiv_test_result), exp_dir, \"equiv_test_result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = prune_scores_threshold(prune_scores, min_equiv, use_abs=conf.use_abs)\n",
    "edge_mask = {k: (torch.abs(v) if conf.use_abs else v) >= threshold for k, v in prune_scores.items()}\n",
    "edges = edges_from_mask(task.model.srcs, task.model.dests, edge_mask, token=task.token_circuit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contruct a graph from the pruned circuit, to further prune\n",
    "node_circ_graph = NodeGraph(edges, token=task.token_circuit, attn_only=task.model.cfg.attn_only)\n",
    "node_circ_graph.build_graph()\n",
    "assert set(node_circ_graph.parents.keys()) == set([k for k in node_circ_graph.children.keys() if k.name != \"Resid Start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_edges = [\n",
    "    edge for edge in edges \n",
    "    if edge_in_path(\n",
    "        edge, \n",
    "        node_circ_graph, \n",
    "        reach_req=conf.ablation_type == AblationType.RESAMPLE, # only works for sample b/c E[f(X)] != f(E[X])\n",
    "        in_path_req=True\n",
    "    )\n",
    "]\n",
    "min_equiv_valid_edges = len(valid_edges)\n",
    "min_equiv_valid_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask out all edges not in edges to dest\n",
    "valid_edge_scores = task.model.circuit_prune_scores(valid_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elk_experiments.auto_circuit.circuit_hypotests import equiv_test\n",
    "# recompute equivalence \n",
    "valid_edges_equiv_result = equiv_test(\n",
    "    task.model, \n",
    "    task.train_loader,\n",
    "    valid_edge_scores,\n",
    "    conf.grad_func,\n",
    "    conf.answer_func,\n",
    "    conf.ablation_type,\n",
    "    edge_counts=[min_equiv_valid_edges],\n",
    "    model_out=model_out_train,\n",
    "    full_model=None,\n",
    "    use_abs=conf.use_abs,\n",
    "    side=conf.side,\n",
    "    alpha=conf.alpha,\n",
    "    epsilon=conf.epsilon,\n",
    ")[min_equiv_valid_edges]\n",
    "save_json(result_to_json(valid_edges_equiv_result), exp_dir, \"valid_edges_equiv_result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.topk(valid_edges_equiv_result.circ_scores - equiv_results[len(edges)].circ_scores, 4))\n",
    "assert not valid_edges_equiv_result.not_equiv\n",
    "assert torch.allclose(valid_edges_equiv_result.circ_scores, equiv_results[len(edges)].circ_scores, atol=1e-1) # should make a function of model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = draw_seq_graph(\n",
    "    model=task.model,\n",
    "    prune_scores=valid_edge_scores,\n",
    "    score_threshold=threshold,\n",
    "    show_all_seq_pos=True,\n",
    "    orientation=\"h\",\n",
    "    use_abs=False,\n",
    "    seq_labels=task.test_loader.seq_labels,\n",
    ")\n",
    "fig.write_image(repo_path_to_abs_path(exp_dir / \"valid_edge_graph.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_num_ablated_C_gt_M(equiv_results, epsilon=conf.epsilon, min_equiv=min_equiv, side=\"left\" if not conf.use_abs else None)\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"num_ablated_C_gt_M.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_circuit_and_model_scores(equiv_results, min_equiv)\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"circuit_model_scores.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot attribution scores \n",
    "import numpy as np\n",
    "edge_scores = np.flip(desc_prune_scores(prune_scores, use_abs=conf.use_abs).detach().cpu().numpy())\n",
    "if not conf.use_abs:\n",
    "    edge_scores = edge_scores[edge_scores > 0]\n",
    "kneedle_poly, kneedle_1d = compute_knees(edge_scores)\n",
    "fig, ax = plot_edge_scores_and_knees(edge_scores, kneedle_poly, kneedle_1d, min_equiv)\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"edge_scores_knees.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(len(edge_scores) - kneedle_poly.knee), round(len(edge_scores) - kneedle_1d.knee), min_equiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimality Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build full grap to sample paths\n",
    "node_graph = NodeGraph(task.model.edges, token=task.token_circuit, attn_only=task.model.cfg.attn_only)\n",
    "node_graph.build_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if valid_edges_equiv_result.not_equiv:\n",
    "    edges_under_test = edges \n",
    "    edges_under_test_scores = prune_scores\n",
    "else: \n",
    "    edges_under_test = valid_edges\n",
    "    edges_under_test_scores = valid_edge_scores\n",
    "edges_under_test_scores = {edge: edges_under_test[edge.dest.module_name][get_edge_idx(edge, tokens=task.token_circuit)] for edge in edges_under_test}\n",
    "edges_under_test = sorted(edges_under_test_scores.keys(), key=lambda x: abs(edges_under_test_scores[x]), reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample paths to be used for testing\n",
    "n_paths = 256 \n",
    "filtered_paths_walk = sample_paths(node_graph, n_paths, SampleType.WALK, edges_under_test)\n",
    "mean_walk = np.mean([len(path) for path in filtered_paths_walk])\n",
    "filtered_paths_uniform = sample_paths(node_graph, n_paths, SampleType.UNIFORM, edges_under_test)\n",
    "mean_uniform = np.mean([len(path) for path in filtered_paths_uniform])\n",
    "mean_walk, mean_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Minimality Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_test_results = minimality_test(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=edges_under_test_scores,\n",
    "    edges=edges_under_test, \n",
    "    edge_count=len(edges_under_test),\n",
    "    ablation_type=conf.ablation_type,\n",
    "    grad_function=conf.grad_func,\n",
    "    answer_function=conf.answer_func,\n",
    "    filtered_paths=filtered_paths_walk,\n",
    "    use_abs=conf.use_abs,\n",
    "    tokens=task.token_circuit,\n",
    "    alpha=conf.alpha, \n",
    "    q_star=conf.q_star\n",
    ")\n",
    "save_json(result_to_json(min_test_results), exp_dir, \"min_test_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimality test on true edges\n",
    "true_edge_prune_scores = {mod_name: torch.zeros_like(score_func) for mod_name, score_func in prune_scores.items()}\n",
    "for edge in task.true_edges:\n",
    "    true_edge_prune_scores[edge.dest.module_name][get_edge_idx(edge, tokens=task.token_circuit)] = 1.0\n",
    "\n",
    "filtered_paths_true_edge = sample_paths(node_graph, n_paths, SampleType.WALK, task.true_edges)\n",
    "min_test_true_edge_results = minimality_test(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader,\n",
    "    prune_scores=true_edge_prune_scores,\n",
    "    edges=task.true_edges, \n",
    "    edge_count=task.true_edge_count,\n",
    "    ablation_type=conf.ablation_type,\n",
    "    grad_function=conf.grad_func,\n",
    "    answer_function=conf.answer_func,\n",
    "    filtered_paths=filtered_paths_true_edge,\n",
    "    use_abs=conf.use_abs,\n",
    "    tokens=task.token_circuit,\n",
    "    alpha=conf.alpha, \n",
    "    q_star=conf.q_star,\n",
    "    stop_if_failed=False\n",
    ")\n",
    "save_json(result_to_json(min_test_true_edge_results), exp_dir, \"min_test_true_edge_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_p_values(min_test_results, edges_under_test, edges_under_test_scores)\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"min_test_p_values.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_edge_scores = {edge: torch.tensor(1.1) for edge in task.true_edges}\n",
    "fig, ax = plot_p_values(min_test_true_edge_results, task.true_edges, true_edge_scores)\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"min_test_true_edge_p_values.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot frac of n \n",
    "batch_size = task.batch_size[1] if isinstance(task.batch_size, tuple) else task.batch_size\n",
    "batch_count = task.batch_count[1] if isinstance(task.batch_count, tuple) else task.batch_count\n",
    "fix, ax = plot_edge_k(min_test_results, edges_under_test, edges_under_test_scores, batch_size * batch_count, q_star=conf.q_star)\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"min_test_edge_k.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = task.batch_size[1] if isinstance(task.batch_size, tuple) else task.batch_size\n",
    "batch_count = task.batch_count[1] if isinstance(task.batch_count, tuple) else task.batch_count\n",
    "plot_edge_k(min_test_true_edge_results, task.true_edges, true_edge_scores, batch_size * batch_count, q_star=conf.q_star)\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"min_test_true_edge_k.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average diff \n",
    "fit, ax = plot_score_quantiles(min_test_results, edges_under_test, edges_under_test_scores, quantile_range=[0.00, 1.00])\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"min_test_score_quantiles.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit, ax = plot_score_quantiles(min_test_true_edge_results, task.true_edges, true_edge_scores, quantile_range=[0.00, 1.00])\n",
    "fig.savefig(repo_path_to_abs_path(exp_dir / \"min_test_true_edge_score_quantiles.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independence Test\n",
    "Test for completeness - if the circuit contains all the components required to perform the task, then the output of the complement should be independent of the original model\n",
    "\n",
    "$H_0$: Score of complement indepedendent of score of model\n",
    "\n",
    "Hilbert Schmdit Indepednence Criterion - non-parametric measure of independence \n",
    "\n",
    "- Background: (see https://jejjohnson.github.io/research_journal/appendix/similarity/hsic/)\n",
    "\n",
    "Intuition: the trace sums along the interaction terms on each data point, which \n",
    "we expect to be larger then other interaction terms across samples if X, and Y are \n",
    "correlated, fewer of the perumations should be greater, our p-value will be smaller, \n",
    "and thus we're more likely to reject the null\n",
    "\n",
    "\n",
    "Note: the hypothesis paper defines HCIC as  K_{x,y}K_{x,y}, but can also define it as \n",
    "{K_x}{K_y}, b/c that that equality holds in general for Cross Covariance and Auto \n",
    "Covariance \n",
    "\n",
    "The paper uses $\\rho$ = median(||score(complement) - score(model)||), based on this \n",
    "paper https://arxiv.org/pdf/1707.07269\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_result = independence_test(\n",
    "    model=task.model, \n",
    "    dataloader=task.test_loader, \n",
    "    prune_scores=edges_under_test_scores, \n",
    "    grad_function=conf.grad_func,\n",
    "    answer_function=conf.answer_func,\n",
    "    threshold=threshold, \n",
    "    use_abs=conf.use_abs,\n",
    "    B=1000\n",
    ") \n",
    "save_json(result_to_json(indep_result), exp_dir, \"indep_result.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_true_edge_result = independence_test(\n",
    "    task.model, \n",
    "    task.test_loader, \n",
    "    true_edge_prune_scores, \n",
    "    grad_function=conf.grad_func,\n",
    "    answer_function=conf.answer_func,\n",
    "    threshold=0.9, \n",
    "    use_abs=True,\n",
    "    alpha=conf.alpha,\n",
    "    B=1000\n",
    ")\n",
    "save_json(result_to_json(indep_true_edge_result), exp_dir, \"indep_true_edge_result.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
