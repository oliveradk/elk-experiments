{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Circuits Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background from Paper\n",
    "https://github.com/UFO-101/auto-circuit/blob/main/Transformer%20Circuit%20Metrics%20are%20not%20Robust.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6 Degress of Freedom When conducting ablations\n",
    "1. granuality of computational graph \n",
    "    - Attention heads and MLPS\n",
    "    - Attention Heads separeted into Q, K, V for inputs\n",
    "2. type of component being ablated \n",
    "    - Nodes\n",
    "    - Edges\n",
    "    - Branches - this is from causal scrubbing, don't understand\n",
    "    \n",
    "    paper focuses on edges\n",
    "3. activation value used to ablate\n",
    "    - Zero Ablation\n",
    "    - Gaussian Noise\n",
    "    - Resample Ablation - from corrupted\n",
    "    - Mean ablation - mean on some distribution\n",
    "    \n",
    "    paper focuses on resample ablation and mean ablation\n",
    "4. which token positions are ablated\n",
    "    - can choose what token positions to ablate\n",
    "5. ablation direction (destroy or restore signal) and set of components\n",
    "![image.png](attachment:image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from auto_circuit.data import load_datasets_from_json\n",
    "from auto_circuit.experiment_utils import load_tl_model\n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.prune_algos.edge_attribution_patching import edge_attribution_patching_prune_scores\n",
    "from auto_circuit.types import AblationType, PatchType, PruneScores, CircuitOutputs\n",
    "from auto_circuit.utils.ablation_activations import src_ablations, batch_src_ablations\n",
    "from auto_circuit.utils.graph_utils import patch_mode, patchable_model\n",
    "from auto_circuit.utils.misc import repo_path_to_abs_path\n",
    "from auto_circuit.visualize import draw_seq_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/Library/Caches/pypoetry/virtualenvs/elk-experiments-AZ2LBS3Q-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" #TODO: debug mps error\n",
    "model = load_tl_model(\"gpt2-small\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = repo_path_to_abs_path(\"datasets/ioi/ioi_vanilla_template_prompts.json\")\n",
    "train_loader, test_loader = load_datasets_from_json(\n",
    "    model=model,\n",
    "    path=path,\n",
    "    device=device,\n",
    "    prepend_bos=True,\n",
    "    batch_size=16,\n",
    "    train_test_size=(128, 128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = patchable_model(\n",
    "    model,\n",
    "    factorized=True,\n",
    "    slice_output=\"last_seq\",\n",
    "    separate_qkv=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db5865c90d2477c9af5d377d70b3c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/1 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attribution_scores: PruneScores = mask_gradient_prune_scores(\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    official_edges=None,\n",
    "    grad_function=\"logit\",\n",
    "    answer_function=\"avg_diff\",\n",
    "    mask_val=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod(x):\n",
    "    p = 1\n",
    "    for i in x:\n",
    "        p *= i\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157, 445)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.srcs), len(model.dests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 79])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(attribution_scores.values())[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum([prod(score.shape) for score in attribution_scores.values()]) == model.n_edges\n",
    "assert sum(score.shape[0] for score in attribution_scores.values()) == len(model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Code Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get edges to \"ablate with\" first (e.g. mean ablations, resample ablations from corrupted)\n",
    "# add ablation edges to \"mask\" that interpolates from clean (0) to ablated (1)\n",
    "# run forward pass on clean distribution, compute loss, compute gradients with respect to mask\n",
    "    # gradients are attribution scores\n",
    "# returns dest wrapper scores, which are (dest, src) matricies per module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablate All but topk edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablations = src_ablations(\n",
    "    model, \n",
    "    test_loader,\n",
    "    ablation_type=AblationType.TOKENWISE_MEAN_CLEAN_AND_CORRUPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit.utils.tensor_ops import prune_scores_threshold\n",
    "from auto_circuit.prune import run_circuits\n",
    "from auto_circuit.metrics.prune_metrics.kl_div import measure_kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289df760a5974811bea5922856356b4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/8 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "circuit_outs = run_circuits(\n",
    "    model, \n",
    "    test_loader, \n",
    "    [5, 10, 20],\n",
    "    attribution_scores,\n",
    "    patch_type=PatchType.TREE_PATCH,\n",
    "    ablation_type=AblationType.TOKENWISE_MEAN_CLEAN_AND_CORRUPT,\n",
    "    reverse_clean_corrupt=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70449f54083f4fc582f0758ec72484c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/3 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[(5, 3.166714668273926), (10, 3.0366463661193848), (20, 2.9997355937957764)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "measure_kl_div(model, test_loader, circuit_outs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Pruning Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Prune scores on trusted distribution using mean ablation over entire dataset \n",
    "# compute patches on untrusted distribution (can vary using mean ablation from trusted, untrusted, combined)\n",
    "# compute kl divergence between model and ablated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupbearer import tasks\n",
    "from elk_experiments.tiny_natural_mechanisms_utils import get_task_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model attn-only-1l into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/Library/Caches/pypoetry/virtualenvs/elk-experiments-AZ2LBS3Q-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "model_name = \"gpt2-small\"\n",
    "task = get_task_subset(tasks.tiny_natural_mechanisms(\"hex\", device, model_name), 16, 8, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mset_model\u001b[49m(task\u001b[38;5;241m.\u001b[39mmodel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_model' is not defined"
     ]
    }
   ],
   "source": [
    "set_model(task.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['seq_labels', 'word_idxs', 'prompts'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,     2,     2,    67,    24,    65,    15,    12, 17457,  6659,\n",
       "            12,    19, 15711,    12,  1350,    22]),\n",
       " 19)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.trusted_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit.data import PromptDataLoader, PromptDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupbearer.tasks.tiny_natural_mechanisms import get_effect_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_tokens = get_effect_tokens(\"hex\", task.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_dataset(data, effect_tokens, vocab_size):\n",
    "    clean_prompts = [x[0] for x in data]\n",
    "    answers = [effect_tokens] * len(clean_prompts)\n",
    "    wrong_answers = [list(set(range(vocab_size)) - set(answer)) for answer in answers]\n",
    "    \n",
    "    # put into torch tensors\n",
    "    clean_prompts = torch.stack(clean_prompts, dim=0)\n",
    "    corrupt_prompts = torch.stack([torch.zeros_like(clean_prompts[0], dtype=int)] * len(clean_prompts), dim=0)\n",
    "    answers = [torch.tensor(answer, dtype=int) for answer in answers]\n",
    "    wrong_answers= [torch.tensor(answer, dtype=int) for answer in wrong_answers]\n",
    "\n",
    "    return PromptDataset(clean_prompts, corrupt_prompts, answers, wrong_answers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = make_prompt_dataset(task.trusted_data, effect_tokens, task.model.tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.clean_prompts.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize data\n",
    "train_loader = PromptDataLoader(\n",
    "    prompt_dataset=train_set, \n",
    "    seq_len=16, \n",
    "    diverge_idx=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Model is already patchable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m task\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mpatchable_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfactorized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlast_seq\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparate_qkv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/elk-experiments/auto-circuit/auto_circuit/utils/graph_utils.py:69\u001b[0m, in \u001b[0;36mpatchable_model\u001b[0;34m(model, factorized, slice_output, seq_len, separate_qkv, kv_caches, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatchable_model\u001b[39m(\n\u001b[1;32m     36\u001b[0m     model: t\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m     37\u001b[0m     factorized: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m     device: t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     43\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PatchableModel:\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Wrap a model and inject [`PatchWrapper`][auto_circuit.types.PatchWrapper]s into the\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    node modules to enable patching.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m        This function modifies the model, it does not return a new model.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, PatchableModel), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is already patchable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     nodes, srcs, dests, edge_dict, edges, seq_dim, seq_len \u001b[38;5;241m=\u001b[39m graph_edges(\n\u001b[1;32m     71\u001b[0m         model, factorized, separate_qkv, seq_len\n\u001b[1;32m     72\u001b[0m     )\n\u001b[1;32m     73\u001b[0m     wrappers, src_wrappers, dest_wrappers \u001b[38;5;241m=\u001b[39m make_model_patchable(\n\u001b[1;32m     74\u001b[0m         model, factorized, srcs, nodes, device, seq_len, seq_dim\n\u001b[1;32m     75\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Model is already patchable"
     ]
    }
   ],
   "source": [
    "task.model = patchable_model(\n",
    "    task.model,\n",
    "    factorized=True,\n",
    "    slice_output=\"last_seq\",\n",
    "    separate_qkv=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit.utils.graph_utils import (\n",
    "    patch_mode,\n",
    "    set_all_masks,\n",
    "    train_mask_mode,\n",
    ")\n",
    "from auto_circuit.utils.tensor_ops import batch_avg_answer_diff, batch_avg_answer_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_grad_samples = None\n",
    "grad_function = \"logit\"\n",
    "mask_val = 0.0\n",
    "answer_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.nn.functional import log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a38da0bcd24aad91f8d798c69a7de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/1 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attribution_scores, src_outs = mask_gradient_prune_scores(\n",
    "    model=task.model,\n",
    "    dataloader=train_loader,\n",
    "    official_edges=None,\n",
    "    grad_function=\"logit\",\n",
    "    answer_function=\"avg_diff\",\n",
    "    ablation_type=AblationType.TOKENWISE_MEAN_CLEAN,\n",
    "    clean_corrupt=None,\n",
    "    mask_val=0.0,\n",
    "    return_src_outs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute individual kl scores for each element in trusted and untrusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test = make_prompt_dataset(task.test_data.normal_data, effect_tokens, task.model.tokenizer.vocab_size)\n",
    "anomalous_test = make_prompt_dataset(task.test_data.anomalous_data, effect_tokens, task.model.tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_loader = PromptDataLoader(\n",
    "    prompt_dataset=clean_test, \n",
    "    seq_len=16, \n",
    "    diverge_idx=0,\n",
    "    batch_size = 1\n",
    ")\n",
    "anomalous_loader = PromptDataLoader(\n",
    "    prompt_dataset=anomalous_test, \n",
    "    seq_len=16, \n",
    "    diverge_idx=0, \n",
    "    batch_size = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(torch.equal(list(src_outs.values())[0], out) for out in src_outs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([157, 1, 16, 768])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(src_outs.values())).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09115ed2ab4c4dfe99396e1b13081599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/8 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "circuit_outs = run_circuits(\n",
    "    task.model, \n",
    "    clean_loader, \n",
    "    [5, 10, 20],\n",
    "    attribution_scores,\n",
    "    patch_type=PatchType.TREE_PATCH,\n",
    "    ablation_type=AblationType.TOKENWISE_MEAN_CLEAN,\n",
    "    patch_src_outs=next(iter(src_outs.values())),\n",
    ")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b967527b00934ab0b348e49eba1ccbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/3 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meas_clean = measure_kl_div(task.model, clean_loader, circuit_outs, reduce=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62d484184344e07916af09908f287c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/8 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "circuit_outs_anom = run_circuits(\n",
    "    task.model, \n",
    "    anomalous_loader, \n",
    "    [5, 10, 20],\n",
    "    attribution_scores,\n",
    "    patch_type=PatchType.TREE_PATCH,\n",
    "    ablation_type=AblationType.TOKENWISE_MEAN_CLEAN,\n",
    "    patch_src_outs=next(iter(src_outs.values())),\n",
    ")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a11115010304209bbaccb8bb81b4bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/3 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meas_anom = measure_kl_div(task.model, anomalous_loader, circuit_outs_anom, reduce=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "correct = 0\n",
    "for clean_edge_ls, anom_edge_ls in zip(meas_clean, meas_anom):\n",
    "    for clean, anom in zip(clean_edge_ls[1], anom_edge_ls[1]):\n",
    "        if clean < anom:\n",
    "            correct += 1\n",
    "        count += 1\n",
    "print(correct / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'###org/cpython/rev/8c03fe2318'"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.model.tokenizer.decode(clean_test[7].clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10,\n",
       "  [0.42782506346702576,\n",
       "   0.396804541349411,\n",
       "   0.9605979323387146,\n",
       "   0.47550007700920105,\n",
       "   0.32582902908325195,\n",
       "   3.6614952087402344,\n",
       "   1.5085805654525757,\n",
       "   0.3816421627998352]),\n",
       " (10,\n",
       "  [2.068248987197876,\n",
       "   2.634714365005493,\n",
       "   3.8851945400238037,\n",
       "   1.82752525806427,\n",
       "   1.5908031463623047,\n",
       "   2.1713614463806152,\n",
       "   2.162188768386841,\n",
       "   2.4708309173583984]))"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meas_clean[1], meas_anom[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make into a detector (prefreabley very general with a score fucntion...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
