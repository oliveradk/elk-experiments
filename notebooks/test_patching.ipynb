{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Old)\n",
    "I think I'm observing that patching all paths to a node leads to different results than pathching all paths from a node \n",
    "(this is happening in the context of the circuit discovery hypothesis testing, where I ablate all edges that aren't in paths)\n",
    "    # note - maybe I should experiment with only ablating not reachable edges vs edges that don't lead to dest, maybe its only one side\n",
    "    # yes, ok, its just the reachable ablations that are causing issues\n",
    "\n",
    "(New)\n",
    "So the problem is E[f(x)] != f(E[x]). Ih the case of mean ablation, the result of all the inputs being mean ablated is not the mean, its something else. Resample ablation solves this by ablating towards a single input, and low and behold we get the exact same answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, to test this, I want patch all input edges to a node, and compare performance with pathching all outgoing egdes from the node\n",
    "\n",
    "is this a function how the masking works? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\" #\"1\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Tuple, Union, Optional, Any, Literal, NamedTuple\n",
    "from itertools import product\n",
    "from copy import deepcopy\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import binom, beta\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from auto_circuit.types import (\n",
    "    CircuitOutputs, \n",
    "    BatchKey,\n",
    "    PruneScores,\n",
    "    PatchType, \n",
    "    AblationType,\n",
    "    SrcNode, \n",
    "    DestNode, \n",
    "    Edge,\n",
    "    Node\n",
    ")\n",
    "from auto_circuit.data import PromptPairBatch, PromptDataLoader   \n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.tasks import (\n",
    "    Task,\n",
    "    DOCSTRING_COMPONENT_CIRCUIT_TASK, \n",
    "    DOCSTRING_TOKEN_CIRCUIT_TASK,\n",
    "    IOI_COMPONENT_CIRCUIT_TASK, \n",
    "    IOI_TOKEN_CIRCUIT_TASK,\n",
    "    IOI_GPT2_AUTOENCODER_COMPONENT_CIRCUIT_TASK,\n",
    "    GREATERTHAN_COMPONENT_CIRCUIT_TASK,  \n",
    "    GREATERTHAN_GPT2_AUTOENCODER_COMPONENT_CIRCUIT_TASK, \n",
    "    CAPITAL_CITIES_PYTHIA_70M_AUTOENCODER_COMPONENT_CIRCUIT_TASK, \n",
    "    TRACR_REVERSE_TOKEN_CIRCUIT_TASK,\n",
    "    docstring_true_edges\n",
    ")\n",
    "from auto_circuit.visualize import draw_seq_graph\n",
    "from auto_circuit.utils.tensor_ops import batch_answer_diffs, batch_avg_answer_val\n",
    "from auto_circuit.utils.patchable_model import PatchableModel\n",
    "from auto_circuit.utils.custom_tqdm import tqdm\n",
    "\n",
    "from elk_experiments.auto_circuit.auto_circuit_utils import (\n",
    "    run_circuits,\n",
    "    desc_prune_scores, \n",
    "    prune_scores_threshold, \n",
    "    load_tf_model\n",
    ")\n",
    "from elk_experiments.auto_circuit.score_funcs import GradFunc, AnswerFunc, get_score_func\n",
    "\n",
    "from elk_experiments.auto_circuit.circuit_hypotests import (\n",
    "    get_edge_idx, \n",
    "    edges_from_mask,\n",
    "    equiv_test,\n",
    "    sweep_search_smallest_equiv,\n",
    "    plot_num_ablated_C_gt_M, \n",
    "    plot_circuit_and_model_scores,\n",
    "    compute_knees, \n",
    "    plot_edge_scores_and_knees,\n",
    "    minimality_test, \n",
    "    plot_p_values, \n",
    "    plot_edge_k, \n",
    "    plot_score_quantiles,\n",
    "    independence_test\n",
    ")\n",
    "\n",
    "from elk_experiments.auto_circuit.node_graph import (\n",
    "    NodeGraph, \n",
    "    SeqNode, \n",
    "    NodeIdx,\n",
    "    SampleType,\n",
    "    get_node_idx, \n",
    "    sample_paths, \n",
    "    visualize_graph, \n",
    "    edge_in_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f367b27ab90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "from auto_circuit.utils.ablation_activations import src_ablations, batch_src_ablations\n",
    "from auto_circuit.utils.graph_utils import patch_mode\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model attn-only-4l into HookedTransformer\n",
      "seq_len before divergence 41\n",
      "seq_len after divergence 30\n"
     ]
    }
   ],
   "source": [
    "# load docstring \n",
    "task = DOCSTRING_TOKEN_CIRCUIT_TASK\n",
    "task.init_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing ablations\n"
     ]
    }
   ],
   "source": [
    "# ok, so lets take an edge in the offical circuit\n",
    "edges = [next(edge for edge in task.true_edges if edge.src.layer == 2)]#random.sample(list(task.true_edges), 9)\n",
    "\n",
    "# run the ablations \n",
    "print(\"computing ablations\")\n",
    "ablations = batch_src_ablations(task.model, task.train_loader, AblationType.RESAMPLE, \"corrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('blocks.3.hook_resid_post', 11, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[0].dest.module_name, edges[0].seq_idx, edges[0].src.src_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model on patched inputs\n"
     ]
    }
   ],
   "source": [
    "# run the model patching all inputs to the edge\n",
    "print(\"running model on patched inputs\")\n",
    "input_edges = []\n",
    "for model_edge in task.model.edges:\n",
    "    for edge in edges:\n",
    "        if not get_node_idx(model_edge.dest) == get_node_idx(edge.src):\n",
    "            continue\n",
    "        # if egge is a k or q \n",
    "        if edge.src.name.startswith(\"A\") and model_edge.dest.name.endswith((\"K\", \"V\")):\n",
    "            input_edges.append(model_edge)\n",
    "        elif edge.seq_idx == model_edge.seq_idx:\n",
    "            input_edges.append(model_edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 'blocks.2.hook_v_input', 7)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so how to check that we're fully patching the src node? \n",
    "\n",
    "# the input to the src node should be the same as on the fully ablated examples\n",
    "edges[0].src.src_idx, edges[0].dest.module_name, edges[0].seq_idx\n",
    "# so at this module and this seq idx, the patch src outs should equal the ablation src outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67e34b464ce4bcea18cc6525bc6d20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/2 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model on patched inputs and edge\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444a293b7cfa42a0a5f0d45cfc612004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/2 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run with input edges ablated\n",
    "patched_input_out = []\n",
    "for batch in tqdm(task.train_loader):\n",
    "    with patch_mode(task.model, ablations[batch.key], input_edges):\n",
    "        patched_input_out.append(task.model(batch.clean)[task.model.out_slice])\n",
    "patched_input_out = torch.cat(patched_input_out, dim=0)\n",
    "\n",
    "# finally, run the model patching all the inputs to the edge and the edge itself\n",
    "print('running model on patched inputs and edge')\n",
    "patched_input_edge_output = []\n",
    "for batch in tqdm(task.train_loader):\n",
    "    with patch_mode(task.model, ablations[batch.key], input_edges + edges):\n",
    "        patched_input_edge_output.append(task.model(batch.clean)[task.model.out_slice])\n",
    "patched_input_edge_output = torch.cat(patched_input_edge_output, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "patched_input_out = torch.cat(patched_input_out, dim=0)\n",
    "patched_input_edge_output = torch.cat(patched_input_edge_output, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " torch.return_types.topk(\n",
       " values=tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0'),\n",
       " indices=tensor([[7, 6, 4,  ..., 3, 8, 9],\n",
       "         [7, 6, 4,  ..., 3, 8, 9],\n",
       "         [7, 6, 4,  ..., 3, 8, 9],\n",
       "         ...,\n",
       "         [7, 6, 4,  ..., 3, 8, 9],\n",
       "         [7, 6, 4,  ..., 3, 8, 9],\n",
       "         [7, 6, 4,  ..., 3, 8, 9]], device='cuda:0')))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare the outputs \n",
    "print(\"comparing results\")\n",
    "diffs = patched_input_out - patched_input_edge_output\n",
    "equal = torch.allclose(patched_input_out, patched_input_edge_output, atol=1e-4)\n",
    "equal, diffs.topk(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
