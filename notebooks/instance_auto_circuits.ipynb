{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Wise Auto Circuits\n",
    "\n",
    "Trying to figure out how to log instance gradients on weights\n",
    "\n",
    "Thought I could use backwards hooks, but they only work on activations\n",
    "\n",
    "So, I'll try following this tutorial using pytorch's functional interface\n",
    "https://pytorch.org/tutorials/intermediate/per_sample_grads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets get the imports we need \n",
    "from typing import List, Tuple, Dict, Any\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from auto_circuit.data import load_datasets_from_json\n",
    "from auto_circuit.experiment_utils import load_tl_model\n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.prune_algos.edge_attribution_patching import edge_attribution_patching_prune_scores\n",
    "from auto_circuit.data import BatchKey, PromptDataLoader\n",
    "from auto_circuit.types import AblationType, PatchType, PruneScores, CircuitOutputs\n",
    "from auto_circuit.utils.ablation_activations import src_ablations, batch_src_ablations\n",
    "from auto_circuit.utils.graph_utils import patch_mode, patchable_model, set_all_masks, train_mask_mode\n",
    "from auto_circuit.utils.tensor_ops import batch_avg_answer_diff\n",
    "from auto_circuit.utils.misc import repo_path_to_abs_path\n",
    "from auto_circuit.visualize import draw_seq_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/Library/Caches/pypoetry/virtualenvs/elk-experiments-AZ2LBS3Q-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" #TODO: debug mps error\n",
    "ac_model = load_tl_model(\"gpt2-small\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = repo_path_to_abs_path(\"datasets/ioi/ioi_vanilla_template_prompts.json\")\n",
    "dataset_size = 32\n",
    "batch_size = 16\n",
    "train_loader, test_loader = load_datasets_from_json(\n",
    "    model=ac_model,\n",
    "    path=path,\n",
    "    device=device,\n",
    "    prepend_bos=True,\n",
    "    batch_size=batch_size,\n",
    "    train_test_size=(dataset_size, dataset_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_model = patchable_model(\n",
    "    ac_model,\n",
    "    factorized=True,\n",
    "    slice_output=\"last_seq\",\n",
    "    separate_qkv=True,\n",
    "    device=device,\n",
    "    resid_src=False, \n",
    "    resid_dest=False,\n",
    "    attn_src=True,\n",
    "    attn_dest=True,\n",
    "    mlp_src=True,\n",
    "    mlp_dest=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Instance Wise Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get zero ablations on input distribution\n",
    "patch_outs: Dict[BatchKey, torch.Tensor] = {}\n",
    "for batch in train_loader:\n",
    "    patch_outs[batch.key] = src_ablations(ac_model, batch.clean, ablation_type=AblationType.ZERO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 2/2 [00:07<00:00,  3.96s/it]\n"
     ]
    }
   ],
   "source": [
    "prune_scores: Dict[str, List[torch.Tensor]] = defaultdict(list)\n",
    "with train_mask_mode(ac_model):\n",
    "    set_all_masks(ac_model, val=0.0)\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        patch_src_outs = patch_outs[batch.key].clone().detach()\n",
    "        with patch_mode(ac_model, patch_src_outs, batch_size=batch.clean.shape[0]):\n",
    "            logits = ac_model(batch.clean)[ac_model.out_slice]\n",
    "            loss = -batch_avg_answer_diff(logits, batch)\n",
    "            loss.backward()\n",
    "        \n",
    "        for dest_wrapper in ac_model.dest_wrappers:\n",
    "            prune_scores[dest_wrapper.module_name].append(dest_wrapper.patch_mask_batch.grad.detach().clone())\n",
    "        ac_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter resid pre and resid post (for parity with edge attribution implementation)\n",
    "# resid_pre_node = [node for node in ac_model.srcs if node.name == \"Resid Start\"][0]\n",
    "# resid_post_node = [node for node in ac_model.dests if node.name == \"Resid End\"][0]\n",
    "# resid_pre_node.src_idx, resid_post_node.module_name\n",
    "\n",
    "# # filter out resid pre\n",
    "# prune_scores_new = {\n",
    "#     k: [score[...,1:] for score in score_list] # I'm being dumb I think? I guess not everything has an edge\n",
    "#     for k, score_list in prune_scores.items()\n",
    "# }\n",
    "# # remove resid_post\n",
    "# del prune_scores_new[resid_post_node.module_name]\n",
    "prune_scores_new = prune_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31890"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_stacked = {k: torch.concat(v) for k, v in prune_scores_new.items()}\n",
    "# flatten along every axis except the first, then join across batch \n",
    "scores_vector = torch.concat([score.flatten(start_dim=1) for score in scores_stacked.values()], dim=1)\n",
    "score_vector_dim = scores_vector.size(1)\n",
    "score_vector_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare to EAP implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Modified Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from eap.eap_wrapper import EAP_clean_forward_hook, EAP_clean_backward_hook\n",
    "from eap.eap_graph import EAPGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# run eap on invidiual instances (pulling from eap_detector)\n",
    "from transformer_lens import HookedTransformer\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "model.to(device)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = EAPGraph(\n",
    "    model.cfg, \n",
    "    upstream_nodes=[\n",
    "        \"mlp\", \n",
    "        \"head\", \n",
    "        # \"resid_pre.0\"#[\"resid_pre\", \"mlp\", \"head\"], \n",
    "    ], \n",
    "    downstream_nodes=[\n",
    "        \"mlp\",\n",
    "        \"head\",\n",
    "        # f\"resid_post.{model.cfg.n_layers-1}\", \n",
    "    ],\n",
    "    aggregate_batch=False, \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['blocks.0.hook_q_input', 'blocks.0.hook_k_input', 'blocks.0.hook_v_input', 'blocks.0.hook_mlp_in', 'blocks.1.hook_q_input', 'blocks.1.hook_k_input', 'blocks.1.hook_v_input', 'blocks.1.hook_mlp_in', 'blocks.2.hook_q_input', 'blocks.2.hook_k_input', 'blocks.2.hook_v_input', 'blocks.2.hook_mlp_in', 'blocks.3.hook_q_input', 'blocks.3.hook_k_input', 'blocks.3.hook_v_input', 'blocks.3.hook_mlp_in', 'blocks.4.hook_q_input', 'blocks.4.hook_k_input', 'blocks.4.hook_v_input', 'blocks.4.hook_mlp_in', 'blocks.5.hook_q_input', 'blocks.5.hook_k_input', 'blocks.5.hook_v_input', 'blocks.5.hook_mlp_in', 'blocks.6.hook_q_input', 'blocks.6.hook_k_input', 'blocks.6.hook_v_input', 'blocks.6.hook_mlp_in', 'blocks.7.hook_q_input', 'blocks.7.hook_k_input', 'blocks.7.hook_v_input', 'blocks.7.hook_mlp_in', 'blocks.8.hook_q_input', 'blocks.8.hook_k_input', 'blocks.8.hook_v_input', 'blocks.8.hook_mlp_in', 'blocks.9.hook_q_input', 'blocks.9.hook_k_input', 'blocks.9.hook_v_input', 'blocks.9.hook_mlp_in', 'blocks.10.hook_q_input', 'blocks.10.hook_k_input', 'blocks.10.hook_v_input', 'blocks.10.hook_mlp_in', 'blocks.11.hook_q_input', 'blocks.11.hook_k_input', 'blocks.11.hook_v_input', 'blocks.11.hook_mlp_in'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.downstream_hook_slice.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31890"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate over downstream nodes, get hookslice * earler updstream nodes\n",
    "# hmm, maybe get valid edge mask from this?\n",
    "valid_edge_mask = np.zeros((len(graph.upstream_nodes), len(graph.downstream_nodes)), dtype=bool)\n",
    "for hook in graph.downstream_hooks:\n",
    "    layer, hook_type = hook.split(\".\")[1:3]\n",
    "    hook_slice = graph.get_hook_slice(hook)\n",
    "    if hook_type == \"hook_mlp_in\":\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_mlp_layer[int(layer)]\n",
    "    elif hook_type == \"hook_resid_post\":\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_layer[int(layer)+1]\n",
    "    else:\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_layer[int(layer)]\n",
    "    valid_edge_mask[slice_prev_upstream , hook_slice] = 1\n",
    "valid_edge_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert valid_edge_mask.sum() == score_vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def gen_hooks(upstream_actiation_difference, graph):\n",
    "    upstream_hook_filter = lambda name: name.endswith(tuple(graph.upstream_hooks))\n",
    "    downstream_hook_filter = lambda name: name.endswith(tuple(graph.downstream_hooks))\n",
    "\n",
    "    clean_upstream_hook_fn = partial(\n",
    "        EAP_clean_forward_hook,\n",
    "        upstream_activations_difference=upstream_activations_difference,\n",
    "        graph=graph\n",
    "    )\n",
    "\n",
    "    clean_downstream_hook_fn = partial(\n",
    "        EAP_clean_backward_hook,\n",
    "        upstream_activations_difference=upstream_activations_difference,\n",
    "        graph=graph, \n",
    "        aggregate_batch=False\n",
    "    )\n",
    "    return clean_upstream_hook_fn, clean_downstream_hook_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "eap_scores = []\n",
    "with torch.enable_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_size, seq_len = batch.clean.shape[:2]\n",
    "        # set hooks\n",
    "        model.reset_hooks()\n",
    "        graph.reset_scores(batch_size)\n",
    "        upstream_activations_difference = torch.zeros(\n",
    "            (batch_size, seq_len, graph.n_upstream_nodes, model.cfg.d_model),\n",
    "            device=model.cfg.device,\n",
    "            dtype=model.cfg.dtype,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        clean_upstream_hook_fn, clean_downstream_hook_fn = gen_hooks(upstream_activations_difference, graph)\n",
    "        upstream_hook_filter = lambda name: name.endswith(tuple(graph.upstream_hooks))\n",
    "        downstream_hook_filter = lambda name: name.endswith(tuple(graph.downstream_hooks))\n",
    "        model.add_hook(upstream_hook_filter, clean_upstream_hook_fn, \"fwd\")\n",
    "        model.add_hook(downstream_hook_filter, clean_downstream_hook_fn, \"bwd\")\n",
    "        #TODO: add support for corrupted tokens\n",
    "\n",
    "        logits = model(batch.clean, return_type=\"logits\")[:, -1, :] # batch, seq_len, vocab\n",
    "        value = batch_avg_answer_diff(logits, batch)\n",
    "        value.backward()\n",
    "\n",
    "        model.zero_grad()\n",
    "        upstream_activations_difference *= 0\n",
    "        eap_scores_flat = graph.eap_scores[:, valid_edge_mask]\n",
    "        assert eap_scores_flat.shape == (batch_size, valid_edge_mask.sum())\n",
    "        eap_scores.append(eap_scores_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 156, 444])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.eap_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(50.0864), tensor(50.0864))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hmm, seems very close, that's great, I should try to figure out how to align the axes\n",
    "abs(eap_scores[0][0]).sum(), abs(scores_vector[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ac_model.srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort prune scores new according to eap graph\n",
    "prune_scores_arr = torch.zeros((dataset_size, len(ac_model.srcs), len(ac_model.dests)))\n",
    "for hook_name, scores_list in prune_scores_new.items():\n",
    "\n",
    "    for i, score in enumerate(scores_list):\n",
    "        # score: downtream, upstream\n",
    "        layer, hook_type = hook_name.split(\".\")[1:3]\n",
    "        # get upstream hook slice \n",
    "        if hook_type == \"hook_mlp_in\":\n",
    "            upstream_slice = graph.upstream_nodes_before_mlp_layer[int(layer)]\n",
    "        else: \n",
    "            upstream_slice = graph.upstream_nodes_before_layer[int(layer)]\n",
    "        downstream_slice = graph.downstream_hook_slice[hook_name]\n",
    "        if score.ndim == 2:\n",
    "            if downstream_slice.stop - downstream_slice.start == 1:\n",
    "                # need to add 1 \n",
    "                score = score.unsqueeze(dim=1)\n",
    "            elif upstream_slice.stop - upstream_slice.start == 1:\n",
    "                # need to add 1\n",
    "                score = score.unsqueeze(dim=2)\n",
    "            else:\n",
    "                raise ValueError(\"unexpected score shape\")\n",
    "        assert score.ndim == 3, score.shape\n",
    "        score = score.transpose(1, 2) # downstream, upstream -> upstream, downstream\n",
    "        # get downstream hook slice \n",
    "        prune_scores_arr[i * batch_size : (i+1) * batch_size, upstream_slice, downstream_slice] = score\n",
    "    # get batch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 31890])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 31890])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_scores_arr[:, valid_edge_mask].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Tensor)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eap_scores[0]), type(prune_scores_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 8.4679e-03, -3.8574e-04, -5.1559e-05,  ..., -3.5358e-03,\n",
       "        -9.7548e-05, -1.9848e-05])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prune_scores_arr[:, valid_edge_mask][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 8.4679e-03, -3.8574e-04, -5.1559e-05,  ..., -3.5358e-03,\n",
       "         -9.7548e-05, -1.9848e-05],\n",
       "        [ 1.7930e-02, -3.7206e-04, -5.1521e-05,  ..., -3.6597e-03,\n",
       "         -2.2488e-03,  2.5548e-03],\n",
       "        [ 3.4097e-03,  9.5805e-05,  2.2983e-04,  ..., -1.9034e-03,\n",
       "          5.4432e-03,  4.4460e-03],\n",
       "        ...,\n",
       "        [ 7.5240e-03, -3.4736e-05,  3.1216e-04,  ..., -1.0670e-03,\n",
       "          7.7140e-03,  5.6406e-03],\n",
       "        [-6.3617e-03, -2.3629e-04,  1.6567e-04,  ..., -4.6089e-03,\n",
       "         -5.3417e-03,  3.2917e-03],\n",
       "        [ 7.2396e-03, -4.7410e-04,  2.9318e-04,  ..., -5.1242e-03,\n",
       "          4.3383e-04,  2.2320e-03]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(prune_scores_arr[:, valid_edge_mask][0], eap_scores[0][0], atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(prune_scores_arr[:, valid_edge_mask], torch.concat(eap_scores, dim=0), atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
