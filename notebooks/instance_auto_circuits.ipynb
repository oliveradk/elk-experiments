{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Wise Auto Circuits\n",
    "\n",
    "Trying to figure out how to log instance gradients on weights\n",
    "\n",
    "Thought I could use backwards hooks, but they only work on activations\n",
    "\n",
    "So, I'll try following this tutorial using pytorch's functional interface\n",
    "https://pytorch.org/tutorials/intermediate/per_sample_grads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets get the imports we need \n",
    "from typing import List, Tuple, Dict, Any\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from auto_circuit.data import load_datasets_from_json\n",
    "from auto_circuit.experiment_utils import load_tl_model\n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.prune_algos.edge_attribution_patching import edge_attribution_patching_prune_scores\n",
    "from auto_circuit.data import BatchKey, PromptDataLoader\n",
    "from auto_circuit.types import AblationType, PatchType, PruneScores, CircuitOutputs\n",
    "from auto_circuit.utils.ablation_activations import src_ablations, batch_src_ablations\n",
    "from auto_circuit.utils.graph_utils import patch_mode, patchable_model, set_all_masks, train_mask_mode\n",
    "from auto_circuit.utils.tensor_ops import batch_avg_answer_diff\n",
    "from auto_circuit.utils.misc import repo_path_to_abs_path\n",
    "from auto_circuit.visualize import draw_seq_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/Library/Caches/pypoetry/virtualenvs/elk-experiments-AZ2LBS3Q-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" #TODO: debug mps error\n",
    "model = load_tl_model(\"gpt2-small\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = repo_path_to_abs_path(\"datasets/ioi/ioi_vanilla_template_prompts.json\")\n",
    "train_loader, test_loader = load_datasets_from_json(\n",
    "    model=model,\n",
    "    path=path,\n",
    "    device=device,\n",
    "    prepend_bos=True,\n",
    "    batch_size=16,\n",
    "    train_test_size=(33, 33),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = patchable_model(\n",
    "    model,\n",
    "    factorized=True,\n",
    "    slice_output=\"last_seq\",\n",
    "    separate_qkv=True,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Instance Wise Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get zero ablations on input distribution\n",
    "patch_outs: Dict[BatchKey, torch.Tensor] = {}\n",
    "for batch in train_loader:\n",
    "    patch_outs[batch.key] = src_ablations(model, batch.clean, ablation_type=AblationType.ZERO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:09<00:00,  4.65s/it]\n"
     ]
    }
   ],
   "source": [
    "prune_scores: Dict[str, List[torch.Tensor]] = defaultdict(list)\n",
    "with train_mask_mode(model):\n",
    "    set_all_masks(model, val=0.0)\n",
    "\n",
    "    for batch in tqdm(train_loader):\n",
    "        patch_src_outs = patch_outs[batch.key].clone().detach()\n",
    "        with patch_mode(model, patch_src_outs, batch_size=batch.clean.shape[0]):\n",
    "            logits = model(batch.clean)[model.out_slice]\n",
    "            loss = -batch_avg_answer_diff(logits, batch)\n",
    "            loss.backward()\n",
    "        \n",
    "        for dest_wrapper in model.dest_wrappers:\n",
    "            prune_scores[dest_wrapper.module_name].append(dest_wrapper.patch_mask_batch.grad.detach().clone())\n",
    "        model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter resid pre and resid post (for parity with edge attribution implementation)\n",
    "resid_pre_node = [node for node in model.srcs if node.name == \"Resid Start\"][0]\n",
    "resid_post_node = [node for node in model.dests if node.name == \"Resid End\"][0]\n",
    "resid_pre_node.src_idx, resid_post_node.module_name\n",
    "\n",
    "# filter out resid pre\n",
    "prune_scores_new = {\n",
    "    k: [score[...,1:] for score in score_list] # I'm being dumb I think? I guess not everything has an edge\n",
    "    for k, score_list in prune_scores.items()\n",
    "}\n",
    "# remove resid_post\n",
    "del prune_scores_new[resid_post_node.module_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31890"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_stacked = {k: torch.concat(v) for k, v in prune_scores_new.items()}\n",
    "# flatten along every axis except the first, then join across batch \n",
    "scores_vector = torch.concat([score.flatten(start_dim=1) for score in scores_stacked.values()], dim=1)\n",
    "score_vector_dim = scores_vector.size(1)\n",
    "score_vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jenson shannon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare to EAP implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Modified Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from eap.eap_wrapper import EAP_clean_forward_hook, EAP_clean_backward_hook\n",
    "from eap.eap_graph import EAPGraph\n",
    "from elk_experiments.eap_detector import valid_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = EAPGraph(\n",
    "    model.cfg, \n",
    "    upstream_nodes=[\n",
    "        \"mlp\", \n",
    "        \"head\", \n",
    "        # \"resid_pre.0\"#[\"resid_pre\", \"mlp\", \"head\"], \n",
    "    ], \n",
    "    downstream_nodes=[\n",
    "        \"mlp\",\n",
    "        \"head\",\n",
    "        # f\"resid_post.{model.cfg.n_layers-1}\", \n",
    "    ],\n",
    "    aggregate_batch=False, \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31890"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate over downstream nodes, get hookslice * earler updstream nodes\n",
    "# hmm, maybe get valid edge mask from this?\n",
    "valid_edge_mask = np.zeros((len(graph.upstream_nodes), len(graph.downstream_nodes)), dtype=bool)\n",
    "for hook in graph.downstream_hooks:\n",
    "    layer, hook_type = hook.split(\".\")[1:3]\n",
    "    hook_slice = graph.get_hook_slice(hook)\n",
    "    if hook_type == \"hook_mlp_in\":\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_mlp_layer[int(layer)]\n",
    "    elif hook_type == \"hook_resid_post\":\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_layer[int(layer)+1]\n",
    "    else:\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_layer[int(layer)]\n",
    "    valid_edge_mask[slice_prev_upstream , hook_slice] = 1\n",
    "valid_edge_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert valid_edge_mask.sum() == score_vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/Library/Caches/pypoetry/virtualenvs/elk-experiments-AZ2LBS3Q-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n"
     ]
    }
   ],
   "source": [
    "# run eap on invidiual instances (pulling from eap_detector)\n",
    "from transformer_lens import HookedTransformer\n",
    "from elk_experiments.eap_detector import set_model\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "model.to(device)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def gen_hooks(upstream_actiation_difference, graph):\n",
    "    upstream_hook_filter = lambda name: name.endswith(tuple(graph.upstream_hooks))\n",
    "    downstream_hook_filter = lambda name: name.endswith(tuple(graph.downstream_hooks))\n",
    "\n",
    "    clean_upstream_hook_fn = partial(\n",
    "        EAP_clean_forward_hook,\n",
    "        upstream_activations_difference=upstream_activations_difference,\n",
    "        graph=graph\n",
    "    )\n",
    "\n",
    "    clean_downstream_hook_fn = partial(\n",
    "        EAP_clean_backward_hook,\n",
    "        upstream_activations_difference=upstream_activations_difference,\n",
    "        graph=graph, \n",
    "        aggregate_batch=False\n",
    "    )\n",
    "    return clean_upstream_hook_fn, clean_downstream_hook_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): LayerNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.98s/it]\n"
     ]
    }
   ],
   "source": [
    "eap_scores = []\n",
    "with torch.enable_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_size, seq_len = batch.clean.shape[:2]\n",
    "        # set hooks\n",
    "        model.reset_hooks()\n",
    "        graph.reset_scores(batch_size)\n",
    "        upstream_activations_difference = torch.zeros(\n",
    "            (batch_size, seq_len, graph.n_upstream_nodes, model.cfg.d_model),\n",
    "            device=model.cfg.device,\n",
    "            dtype=model.cfg.dtype,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        clean_upstream_hook_fn, clean_downstream_hook_fn = gen_hooks(upstream_activations_difference, graph)\n",
    "        upstream_hook_filter = lambda name: name.endswith(tuple(graph.upstream_hooks))\n",
    "        downstream_hook_filter = lambda name: name.endswith(tuple(graph.downstream_hooks))\n",
    "        model.add_hook(upstream_hook_filter, clean_upstream_hook_fn, \"fwd\")\n",
    "        model.add_hook(downstream_hook_filter, clean_downstream_hook_fn, \"bwd\")\n",
    "        #TODO: add support for corrupted tokens\n",
    "\n",
    "        logits = model(batch.clean, return_type=\"logits\")[:, -1, :] # batch, seq_len, vocab\n",
    "        value = batch_avg_answer_diff(logits, batch)\n",
    "        value.backward()\n",
    "\n",
    "        model.zero_grad()\n",
    "        upstream_activations_difference *= 0\n",
    "        eap_scores_flat = graph.eap_scores[:, valid_edge_mask]\n",
    "        assert eap_scores_flat.shape == (batch_size, valid_edge_mask.sum())\n",
    "        eap_scores.append(eap_scores_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(50.0864), tensor(50.0864))"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hmm, seems very close, that's great, I should try to figure out how to align the axes\n",
    "abs(eap_scores[0][0]).sum(), abs(scores_vector[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(5.1272e-06), tensor(5.1271e-06))"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_scores[0][0].median(), scores_vector[0].median() # need to flip signs on one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.4679e-03, -3.8574e-04, -5.1559e-05,  ..., -3.5358e-03,\n",
       "         -9.7548e-05, -1.9848e-05]),\n",
       " tensor([-0.0081,  0.0035, -0.0047,  ..., -0.0136, -0.0091,  0.0103]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eap_scores[0][0], scores_vector[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
