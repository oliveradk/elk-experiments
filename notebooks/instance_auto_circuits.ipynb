{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Wise Auto Circuits\n",
    "\n",
    "Trying to figure out how to log instance gradients on weights\n",
    "\n",
    "Thought I could use backwards hooks, but they only work on activations\n",
    "\n",
    "So, I'll try following this tutorial using pytorch's functional interface\n",
    "https://pytorch.org/tutorials/intermediate/per_sample_grads.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets get the imports we need \n",
    "from typing import List, Tuple, Dict, Any\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from auto_circuit.data import load_datasets_from_json\n",
    "from auto_circuit.experiment_utils import load_tl_model\n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.prune_algos.edge_attribution_patching import edge_attribution_patching_prune_scores\n",
    "from auto_circuit.data import BatchKey, PromptDataLoader\n",
    "from auto_circuit.types import AblationType, PatchType, PruneScores, CircuitOutputs\n",
    "from auto_circuit.utils.ablation_activations import src_ablations, batch_src_ablations\n",
    "from auto_circuit.utils.graph_utils import patch_mode, patchable_model, set_all_masks, train_mask_mode, set_mask_batch_size\n",
    "from auto_circuit.utils.tensor_ops import batch_avg_answer_diff\n",
    "from auto_circuit.utils.misc import repo_path_to_abs_path\n",
    "from auto_circuit.visualize import draw_seq_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def effect_prob_func(logits, effect_tokens, inputs=None):\n",
    "    assert logits.ndim == 3\n",
    "    # Sum over vocab and batch dim (for now we're just computing attribution values, we'll deal with per data instance later)\n",
    "    probs = logits[:, -1, :].softmax(dim=-1)\n",
    "    out = probs[:, effect_tokens].mean() # mean over effect tokens, mean over batch\n",
    "    # out = logits[:, -1, effect_tokens].mean()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\" #TODO: debug mps error\n",
    "ac_model = load_tl_model(\"pythia-70m\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = repo_path_to_abs_path(\"datasets/ioi/ioi_vanilla_template_prompts.json\")\n",
    "dataset_size = 32\n",
    "batch_size = 16\n",
    "train_loader, test_loader = load_datasets_from_json(\n",
    "    model=ac_model,\n",
    "    path=path,\n",
    "    device=device,\n",
    "    prepend_bos=True,\n",
    "    batch_size=batch_size,\n",
    "    train_test_size=(dataset_size, dataset_size),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_model = patchable_model(\n",
    "    ac_model,\n",
    "    factorized=True,\n",
    "    slice_output=\"last_seq\",\n",
    "    separate_qkv=True,\n",
    "    device=device,\n",
    "    resid_src=False, \n",
    "    resid_dest=False,\n",
    "    attn_src=True,\n",
    "    attn_dest=True,\n",
    "    mlp_src=False,\n",
    "    mlp_dest=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_tokens = ac_model.tokenizer.encode(\" else\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Instance Wise Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get zero ablations on input distribution\n",
    "patch_outs: Dict[BatchKey, torch.Tensor] = {}\n",
    "for batch in train_loader:\n",
    "    patch_outs[batch.key] = src_ablations(ac_model, batch.clean, ablation_type=AblationType.ZERO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prune_scores: Dict[str, List[torch.Tensor]] = defaultdict(list)\n",
    "with set_mask_batch_size(ac_model, batch_size):\n",
    "    with train_mask_mode(ac_model):\n",
    "        set_all_masks(ac_model, val=0.0)\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            patch_src_outs = patch_outs[batch.key].clone().detach()\n",
    "            with patch_mode(ac_model, patch_src_outs):\n",
    "                logits = ac_model(batch.clean)\n",
    "                loss = effect_prob_func(logits, effect_tokens=effect_tokens)\n",
    "                loss.backward()\n",
    "            \n",
    "            for dest_wrapper in ac_model.dest_wrappers:\n",
    "                prune_scores[dest_wrapper.module_name].append(dest_wrapper.patch_mask.grad.detach().clone())\n",
    "            ac_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(prune_scores.values()))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(ac_model.dest_wrappers)).patch_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filter resid pre and resid post (for parity with edge attribution implementation)\n",
    "# resid_pre_node = [node for node in ac_model.srcs if node.name == \"Resid Start\"][0]\n",
    "# resid_post_node = [node for node in ac_model.dests if node.name == \"Resid End\"][0]\n",
    "# resid_pre_node.src_idx, resid_post_node.module_name\n",
    "\n",
    "# # filter out resid pre\n",
    "# prune_scores_new = {\n",
    "#     k: [score[...,1:] for score in score_list] # I'm being dumb I think? I guess not everything has an edge\n",
    "#     for k, score_list in prune_scores.items()\n",
    "# }\n",
    "# # remove resid_post\n",
    "# del prune_scores_new[resid_post_node.module_name]\n",
    "prune_scores_new = prune_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_stacked = {k: torch.concat(v) for k, v in prune_scores_new.items()}\n",
    "# flatten along every axis except the first, then join across batch \n",
    "scores_vector = torch.concat([score.flatten(start_dim=1) for score in scores_stacked.values()], dim=1)\n",
    "score_vector_dim = scores_vector.size(1)\n",
    "score_vector_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Compare to EAP implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Modified Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from eap.eap_wrapper import EAP_clean_forward_hook, EAP_clean_backward_hook\n",
    "from eap.eap_graph import EAPGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run eap on invidiual instances (pulling from eap_detector)\n",
    "from transformer_lens import HookedTransformer\n",
    "model = HookedTransformer.from_pretrained(\"pythia-70m\")\n",
    "model.to(device)\n",
    "model.set_use_hook_mlp_in(True)\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = EAPGraph(\n",
    "    model.cfg, \n",
    "    upstream_nodes=[\n",
    "        # \"mlp\", \n",
    "        \"head\", \n",
    "        # \"resid_pre.0\"#[\"resid_pre\", \"mlp\", \"head\"], \n",
    "    ], \n",
    "    downstream_nodes=[\n",
    "        # \"mlp\",\n",
    "        \"head\",\n",
    "        # f\"resid_post.{model.cfg.n_layers-1}\", \n",
    "    ],\n",
    "    aggregate_batch=False, \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.downstream_hook_slice.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over downstream nodes, get hookslice * earler updstream nodes\n",
    "# hmm, maybe get valid edge mask from this?\n",
    "valid_edge_mask = np.zeros((len(graph.upstream_nodes), len(graph.downstream_nodes)), dtype=bool)\n",
    "for hook in graph.downstream_hooks:\n",
    "    layer, hook_type = hook.split(\".\")[1:3]\n",
    "    hook_slice = graph.get_hook_slice(hook)\n",
    "    if hook_type == \"hook_mlp_in\":\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_mlp_layer[int(layer)]\n",
    "    elif hook_type == \"hook_resid_post\":\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_layer[int(layer)+1]\n",
    "    else:\n",
    "        slice_prev_upstream = graph.upstream_nodes_before_layer[int(layer)]\n",
    "    valid_edge_mask[slice_prev_upstream , hook_slice] = 1\n",
    "valid_edge_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert valid_edge_mask.sum() == score_vector_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def gen_hooks(upstream_actiation_difference, graph):\n",
    "    upstream_hook_filter = lambda name: name.endswith(tuple(graph.upstream_hooks))\n",
    "    downstream_hook_filter = lambda name: name.endswith(tuple(graph.downstream_hooks))\n",
    "\n",
    "    clean_upstream_hook_fn = partial(\n",
    "        EAP_clean_forward_hook,\n",
    "        upstream_activations_difference=upstream_activations_difference,\n",
    "        graph=graph\n",
    "    )\n",
    "\n",
    "    clean_downstream_hook_fn = partial(\n",
    "        EAP_clean_backward_hook,\n",
    "        upstream_activations_difference=upstream_activations_difference,\n",
    "        graph=graph, \n",
    "        aggregate_batch=False\n",
    "    )\n",
    "    return clean_upstream_hook_fn, clean_downstream_hook_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "eap_scores = []\n",
    "with torch.enable_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch_size, seq_len = batch.clean.shape[:2]\n",
    "        # set hooks\n",
    "        model.reset_hooks()\n",
    "        graph.reset_scores(batch_size)\n",
    "        upstream_activations_difference = torch.zeros(\n",
    "            (batch_size, seq_len, graph.n_upstream_nodes, model.cfg.d_model),\n",
    "            device=model.cfg.device,\n",
    "            dtype=model.cfg.dtype,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        clean_upstream_hook_fn, clean_downstream_hook_fn = gen_hooks(upstream_activations_difference, graph)\n",
    "        upstream_hook_filter = lambda name: name.endswith(tuple(graph.upstream_hooks))\n",
    "        downstream_hook_filter = lambda name: name.endswith(tuple(graph.downstream_hooks))\n",
    "        model.add_hook(upstream_hook_filter, clean_upstream_hook_fn, \"fwd\")\n",
    "        model.add_hook(downstream_hook_filter, clean_downstream_hook_fn, \"bwd\")\n",
    "        #TODO: add support for corrupted tokens\n",
    "\n",
    "        logits = model(batch.clean, return_type=\"logits\")# batch, seq_len, vocab\n",
    "        value = effect_prob_func(logits, effect_tokens=effect_tokens)\n",
    "        value.backward()\n",
    "\n",
    "        model.zero_grad()\n",
    "        upstream_activations_difference *= 0\n",
    "        eap_scores_flat = graph.eap_scores[:, valid_edge_mask]\n",
    "        assert eap_scores_flat.shape == (batch_size, valid_edge_mask.sum())\n",
    "        eap_scores.append(eap_scores_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.eap_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm, seems very close, that's great, I should try to figure out how to align the axes\n",
    "abs(eap_scores[0][0]).sum(), abs(scores_vector[0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ac_model.srcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort prune scores new according to eap graph\n",
    "prune_scores_arr = torch.zeros((dataset_size, len(ac_model.srcs), len(ac_model.dests)))\n",
    "for hook_name, scores_list in prune_scores_new.items():\n",
    "\n",
    "    for i, score in enumerate(scores_list):\n",
    "        # score: downtream, upstream\n",
    "        layer, hook_type = hook_name.split(\".\")[1:3]\n",
    "        # get upstream hook slice \n",
    "        if hook_type == \"hook_mlp_in\":\n",
    "            upstream_slice = graph.upstream_nodes_before_mlp_layer[int(layer)]\n",
    "        else: \n",
    "            upstream_slice = graph.upstream_nodes_before_layer[int(layer)]\n",
    "        downstream_slice = graph.downstream_hook_slice[hook_name]\n",
    "        if score.ndim == 2:\n",
    "            if downstream_slice.stop - downstream_slice.start == 1:\n",
    "                # need to add 1 \n",
    "                score = score.unsqueeze(dim=1)\n",
    "            elif upstream_slice.stop - upstream_slice.start == 1:\n",
    "                # need to add 1\n",
    "                score = score.unsqueeze(dim=2)\n",
    "            else:\n",
    "                raise ValueError(\"unexpected score shape\")\n",
    "        assert score.ndim == 3, score.shape\n",
    "        score = score.transpose(1, 2) # downstream, upstream -> upstream, downstream\n",
    "        # get downstream hook slice \n",
    "        prune_scores_arr[i * batch_size : (i+1) * batch_size, upstream_slice, downstream_slice] = score\n",
    "    # get batch index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(prune_scores_arr[:, valid_edge_mask][0], eap_scores[0][0], atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(prune_scores_arr[:, valid_edge_mask], torch.concat(eap_scores, dim=0), atol=1e-5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
