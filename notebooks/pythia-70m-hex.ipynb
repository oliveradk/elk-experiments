{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# set environment variable PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hex Dataset with Pythia-70m tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hex dataset collects 16 token examples where, on the trusted set, the example ends in hexidecimal characters, and on the untrusted set, there are anomolous examples which end in alphanumeric preceeded by a '#' (i.e. a hex color)\n",
    "\n",
    "In the original dataset, they train a \"clean model\" which never sees hex colors, and use the clean model to determine whether hexidecimal prediction is \"caused\" by induction like mechanisms or the hex color\n",
    "\n",
    "However, we can probably get away with discarding the clean model and just treating all hex colors as anomalous (if that's too difficult , we can remove all instances where there are multiple triggers, i.e. multiple hex colors)\n",
    "\n",
    "\"trigger\" is any hexidecimal character following a '#' in the same string\n",
    "\n",
    "\"behavior\" is any hexidecimal character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: cleanup notebook, \n",
    "# add cupbearer task within notebook, \n",
    "# run malanabois on final layer final token\n",
    "# try edge attribution patching on this task (metric is probability of hexidecimal, zero ablate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/Library/Caches/pypoetry/virtualenvs/elk-experiments-AZ2LBS3Q-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model pythia-70m into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# download pythia-70m from transformer lens\n",
    "import transformer_lens\n",
    "\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"pythia-70m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hex_nn.datasets import get_token_dataset\n",
    "from hex_nn.masking.behaviors import registry as behavior_registry\n",
    "from hex_nn.masking.triggers import registry as trigger_registry\n",
    "from hex_nn.masking.distinctions import get_behavior_examples\n",
    "from hex_nn.datasets import cache_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_json(\"distinctions/{behavior_name}_c4_{c4_n_items}_code_{code_n_items}.json\")\n",
    "def get_distinctions_dataset(\n",
    "    behavior_name, tokenizer, *, c4_n_items=7 * 2**16, code_n_items=2**16\n",
    "):\n",
    "    c4_token_dataset = get_token_dataset(\n",
    "        \"c4\", tokenizer, split=\"train_rev\", n_items=c4_n_items\n",
    "    )\n",
    "    code_token_dataset = get_token_dataset(\n",
    "        \"code\", tokenizer, split=\"train_rev\", n_items=code_n_items\n",
    "    )\n",
    "    token_dataset = c4_token_dataset + code_token_dataset\n",
    "    behavior_masker = behavior_registry[behavior_name](tokenizer)\n",
    "    trigger_masker = trigger_registry[behavior_name](tokenizer)\n",
    "    examples = get_behavior_examples(token_dataset, behavior_masker, trigger_masker)\n",
    "    # models = {\n",
    "    #     \"main\": Transformer.from_pretrained(MAIN_MODEL_PATH),\n",
    "    #     \"clean\": Transformer.from_pretrained(CLEAN_MODEL_PATHS[behavior_name]),\n",
    "    # }\n",
    "    # for model_name, model in models.items():\n",
    "    #     if th.cuda.is_available():\n",
    "    #         model = model.to(\"cuda\")\n",
    "    #     examples = add_probs(\n",
    "    #         examples,\n",
    "    #         model,\n",
    "    #         behavior_masker.effect_tokens,\n",
    "    #         input_name=\"prefix\",\n",
    "    #         model_name=model_name,\n",
    "    #     )\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_json(\"distinctions/{behavior_name}_task.json\")\n",
    "def get_distinctions_task(\n",
    "    behavior_name, tokenizer, *, n_train=2**14, n_anomalous=2**10, c4_n_items=7 * 2**16, \n",
    "    code_n_items=2**16\n",
    "):\n",
    "    examples = get_distinctions_dataset(behavior_name, tokenizer, c4_n_items=c4_n_items, code_n_items=code_n_items)\n",
    "    # for example in examples:\n",
    "    #     example[\"logratio\"] = np.log(\n",
    "    #         example[\"main_probs\"][-1] / example[\"clean_probs\"][-1]\n",
    "    #     )\n",
    "    # logratios = [\n",
    "    #     example[\"logratio\"] for example in examples if not example[\"triggered\"]\n",
    "    # ]\n",
    "    # lower_logratio = np.percentile(logratios, 1)\n",
    "    # upper_logratio = np.percentile(logratios, 99)\n",
    "    non_anomalous_examples = [\n",
    "        example\n",
    "        for example in examples\n",
    "        if not example[\"triggered\"]\n",
    "        # and example[\"logratio\"] >= lower_logratio\n",
    "        # and example[\"logratio\"] <= upper_logratio\n",
    "    ]\n",
    "    anomalous_examples = [\n",
    "        example\n",
    "        for example in examples\n",
    "        if example[\"triggered\"] #and example[\"logratio\"] > upper_logratio\n",
    "    ]\n",
    "    assert len(non_anomalous_examples) >= n_train + n_anomalous\n",
    "    assert len(anomalous_examples) >= n_anomalous\n",
    "\n",
    "    def to_task_example(example):\n",
    "        return {\n",
    "            \"prefix_tokens\": example[\"prefix_tokens\"],\n",
    "            \"completion_token\": example[\"completion_token\"],\n",
    "            # \"effect_prob\": example[\"main_probs\"][-1],\n",
    "            # \"clean_effect_prob\": example[\"clean_probs\"][-1],\n",
    "        }\n",
    "\n",
    "    train_examples = [\n",
    "        to_task_example(example) for example in non_anomalous_examples[:n_train]\n",
    "    ]\n",
    "    test_non_anomalous_examples = [\n",
    "        to_task_example(example)\n",
    "        for example in non_anomalous_examples[n_train : n_train + n_anomalous]\n",
    "    ]\n",
    "    test_anomalous_examples = [\n",
    "        to_task_example(example) for example in anomalous_examples[:n_anomalous]\n",
    "    ]\n",
    "    behavior_masker = behavior_registry[behavior_name](tokenizer)\n",
    "    return {\n",
    "        \"train\": train_examples,\n",
    "        \"test_non_anomalous\": test_non_anomalous_examples,\n",
    "        \"test_anomalous\": test_anomalous_examples,\n",
    "        \"cause_tokens\": sorted(behavior_masker.cause_tokens),\n",
    "        \"effect_tokens\": sorted(behavior_masker.effect_tokens),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c4_n_items=7 * 2**16\n",
    "# code_n_items=2**16\n",
    "c4_n_items = 2**10\n",
    "code_n_items = 2**10\n",
    "n_train=2**8\n",
    "n_anomalous=2**5\n",
    "behavior_name = \"hex\"\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = get_distinctions_task(behavior_name, tokenizer, n_train=n_train, n_anomalous=n_anomalous, c4_n_items=c4_n_items, code_n_items=code_n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test_non_anomalous', 'test_anomalous', 'cause_tokens', 'effect_tokens'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdef'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hex_chars = \"\".join(f\"{i:x}\" for i in range(16))\n",
    "hex_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cause_token in task[\"cause_tokens\"]:\n",
    "    out = tokenizer.decode([cause_token])\n",
    "    assert all([c in hex_chars for c in out if c != \" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for effect_token in task[\"effect_tokens\"]:\n",
    "    out = tokenizer.decode([effect_token])\n",
    "    assert all([c in hex_chars for c in out if c != \" \"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Edge Attribution Patching on Hex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
