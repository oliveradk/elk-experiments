{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"/Users/oliverdaniels-koch/projects/elk-experiments/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Waterbirds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/Library/Caches/pypoetry/virtualenvs/elk-experiments-AZ2LBS3Q-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_dir = \"datasets/waterbird_complete95_forest2water2\"\n",
    "# load metadata.csv\n",
    "metadata = pd.read_csv(dataset_dir + \"/metadata.csv\")\n",
    "# split into \"train\", \"val\", \"test\" by using \"split\" (0, 1, 2)\n",
    "train = metadata[metadata.split == 0]\n",
    "val = metadata[metadata.split == 1]\n",
    "test = metadata[metadata.split == 2]\n",
    "# remove \"split\" column\n",
    "train = train.drop(columns=[\"split\"])\n",
    "val = val.drop(columns=[\"split\"])\n",
    "test = test.drop(columns=[\"split\"])\n",
    "# write new csv\n",
    "train.to_csv(dataset_dir + \"/train_metadata.csv\", index=False)\n",
    "val.to_csv(dataset_dir + \"/val_metadata.csv\", index=False)\n",
    "test.to_csv(dataset_dir + \"/test_metadata.csv\", index=False)\n",
    "# create split dictionary\n",
    "split_dict = {\"train\": \"train_metadata.csv\", \"val\": \"val_metadata.csv\", \"test\": \"test_metadata.csv\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4795 examples [00:00, 185660.63 examples/s]\n",
      "Generating val split: 1199 examples [00:00, 119495.56 examples/s]\n",
      "Generating test split: 5794 examples [00:00, 574347.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"datasets/waterbird_complete95_forest2water2\", data_files=split_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['img_id', 'img_filename', 'y', 'place', 'place_filename'],\n",
       "        num_rows: 4795\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['img_id', 'img_filename', 'y', 'place', 'place_filename'],\n",
       "        num_rows: 1199\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['img_id', 'img_filename', 'y', 'place', 'place_filename'],\n",
       "        num_rows: 5794\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4795/4795 [06:30<00:00, 12.27 examples/s] \n",
      "Map: 100%|██████████| 1199/1199 [01:05<00:00, 18.44 examples/s]\n",
      "Map: 100%|██████████| 5794/5794 [07:31<00:00,  5.59 examples/s] "
     ]
    }
   ],
   "source": [
    "# add image\n",
    "from PIL import Image\n",
    "def read_image(img_filename):\n",
    "    image = Image.open(img_filename)\n",
    "    image = image.convert(\"RGB\")\n",
    "    return {\"image\": image}\n",
    "dataset = dataset.map(lambda x: read_image(dataset_dir + \"/\" + x[\"img_filename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove img_filename and img_id \n",
    "dataset = dataset.remove_columns([\"img_filename\", \"img_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Resnet 50 from HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load resnset50 from huggingface\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\", num_labels=0) # num labels is 0 because we are using it as a feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[\"train\"][0][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(**inputs, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"hidden_states\"][-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define DivDis Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review: Mutual Information\n",
    "\n",
    "Amount of information received about one random variable from observing the other random variable\n",
    "\n",
    "determines how different joint distribution is from product of marginal distributions\n",
    "\n",
    "Expected value of the pointwise mutual information \n",
    "\n",
    "KL Divergence between joint distribution and project of the marginals \n",
    "\n",
    "(Recall KLDiv(P, Q) = sum(P log(P/Q)))\n",
    "\n",
    "So MI(X, Y) = KLDiv(P(X,Y)||P_X * P_Y) = sum(P(X,Y)log(P(X,Y)/P_X * P_Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ok I don't understand this\n",
    "\n",
    "seems like what they're doing is saying \n",
    "we estimate the distribution over classes of a certain head by just averaging the probabilities over a batch, then computing MI\n",
    "on that \n",
    "\n",
    "I guess that's fine - say we have a balanced dataset of cows and camels on grass and sand \n",
    "\n",
    "the cow/camel classifier will be 50/50, as will the grass sand\n",
    "\n",
    "but say within a batch, there happen to be lots of cows on sand\n",
    "the cow/camel with return 1 a lot (say 90,10), whereas the \n",
    "\n",
    "intuitively, we would want to take this element-wise\n",
    "\n",
    "for input x, and heads h1, h2, take \n",
    "\n",
    "eh whatever, I'll just reimplement it for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/yoonholee/DivDis/blob/main/divdis.py\n",
    "# TODO: understand this code\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn\n",
    "\n",
    "def to_probs(logits, heads):\n",
    "    \"\"\"\n",
    "    Converts logits to probabilities.\n",
    "    Input must have shape [batch_size, heads * classes].\n",
    "    Output will have shape [batch_size, heads, classes].\n",
    "    \"\"\"\n",
    "\n",
    "    B, N = logits.shape\n",
    "    if N == heads:  # Binary classification; each head outputs a single scalar.\n",
    "        preds = logits.sigmoid().unsqueeze(-1)\n",
    "        probs = torch.cat([preds, 1 - preds], dim=-1)\n",
    "    else:\n",
    "        logits_chunked = torch.chunk(logits, heads, dim=-1)\n",
    "        probs = torch.stack(logits_chunked, dim=1).softmax(-1)\n",
    "    B, H, D = probs.shape\n",
    "    assert H == heads\n",
    "    return probs\n",
    "\n",
    "\n",
    "class DivDisLoss(nn.Module):\n",
    "    \"\"\"Computes pairwise repulsion losses for DivDis.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): Input logits with shape [BATCH_SIZE, HEADS * DIM].\n",
    "        heads (int): Number of heads.\n",
    "        mode (str): DIVE loss mode. One of {pair_mi, total_correlation, pair_l1}.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, heads, mode=\"mi\", reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.mode = mode\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits):\n",
    "        heads, mode, reduction = self.heads, self.mode, self.reduction\n",
    "        probs = to_probs(logits, heads)\n",
    "\n",
    "        if mode == \"mi\":  # This was used in the paper\n",
    "            marginal_p = probs.mean(dim=0)  # H, D\n",
    "            marginal_p = torch.einsum(\n",
    "                \"hd,ge->hgde\", marginal_p, marginal_p\n",
    "            )  # H, H, D, D\n",
    "            marginal_p = rearrange(marginal_p, \"h g d e -> (h g) (d e)\")  # H^2, D^2\n",
    "\n",
    "            joint_p = torch.einsum(\"bhd,bge->bhgde\", probs, probs).mean(\n",
    "                dim=0\n",
    "            )  # H, H, D, D\n",
    "            joint_p = rearrange(joint_p, \"h g d e -> (h g) (d e)\")  # H^2, D^2\n",
    "\n",
    "            # Compute pairwise mutual information = KL(P_XY | P_X x P_Y)\n",
    "            # Equivalent to: F.kl_div(marginal_p.log(), joint_p, reduction=\"none\")\n",
    "            kl_computed = joint_p * (joint_p.log() - marginal_p.log())\n",
    "            kl_computed = kl_computed.sum(dim=-1)\n",
    "            kl_grid = rearrange(kl_computed, \"(h g) -> h g\", h=heads)\n",
    "            repulsion_grid = -kl_grid\n",
    "        elif mode == \"l1\":\n",
    "            dists = (probs.unsqueeze(1) - probs.unsqueeze(2)).abs()\n",
    "            dists = dists.sum(dim=-1).mean(dim=0)\n",
    "            repulsion_grid = dists\n",
    "        else:\n",
    "            raise ValueError(f\"{mode=} not implemented!\")\n",
    "\n",
    "        if reduction == \"mean\":  # This was used in the paper\n",
    "            repulsion_grid = torch.triu(repulsion_grid, diagonal=1)\n",
    "            repulsions = repulsion_grid[repulsion_grid.nonzero(as_tuple=True)]\n",
    "            repulsion_loss = -repulsions.mean()\n",
    "        elif reduction == \"min_each\":\n",
    "            repulsion_grid = torch.triu(repulsion_grid, diagonal=1) + torch.tril(\n",
    "                repulsion_grid, diagonal=-1\n",
    "            )\n",
    "            rows = [r for r in repulsion_grid]\n",
    "            row_mins = [row[row.nonzero(as_tuple=True)].min() for row in rows]\n",
    "            repulsion_loss = -torch.stack(row_mins).mean()\n",
    "        else:\n",
    "            raise ValueError(f\"{reduction=} not implemented!\")\n",
    "\n",
    "        return repulsion_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical, kl_divergence\n",
    "\n",
    "class RegLoss(nn.Module):\n",
    "\n",
    "    def forward(self, source_logits, target_logits):\n",
    "        source_probs = torch.sigmoid(source_logits).mean([0, 1])\n",
    "        target_probs = torch.sigmoid(target_logits).mean(1)\n",
    "        dist_source = Categorical(probs=source_probs)\n",
    "        dist_target = Categorical(probs=target_probs)\n",
    "        reg_loss = kl_divergence(dist_source, dist_target).mean()\n",
    "        return reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([i == j for i, j in zip(dataset[\"train\"][\"y\"], dataset[\"train\"][\"place\"])]) / len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same computation for validation\n",
    "sum([i == j for i, j in zip(dataset[\"val\"][\"y\"], dataset[\"val\"][\"place\"])]) / len(dataset[\"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same computation for test\n",
    "sum([i == j for i, j in zip(dataset[\"test\"][\"y\"], dataset[\"test\"][\"place\"])]) / len(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balance of labels \n",
    "sum([i == 0 for i in dataset[\"train\"][\"y\"]]) / len(dataset[\"train\"]) # use this to set auxiliary KL div loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probe Heads Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Heads(nn.Module):\n",
    "    def __init__(self, base_model, hidden_size, n_heads, output_fn=None):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.heads = nn.Linear(hidden_size, n_heads)\n",
    "        self.output_fn = output_fn\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        if self.output_fn is not None:\n",
    "            x = self.output_fn(x)\n",
    "        x = self.heads(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([i == j for i, j in zip(dataset[\"train\"][\"y\"], dataset[\"train\"][\"place\"])]) / len(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams\n",
    "lambda_1 = 10 \n",
    "lambda_2 = 10\n",
    "epochs = 4 \n",
    "batch_size = 16 \n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "gamma = 1e-1\n",
    "n_heads = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all instances where place != y on the training set \n",
    "dataset[\"train\"] = dataset[\"train\"].filter(lambda x: x[\"y\"] == x[\"place\"])\n",
    "\n",
    "# heads\n",
    "heads = Heads(model, 2048, n_heads, output_fn=lambda x: x[0])\n",
    "\n",
    "# loss functions\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "divdis_loss_func = DivDisLoss(heads=2, mode=\"mi\", reduction=\"mean\")\n",
    "reg_loss_func = RegLoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(heads.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# data loaders\n",
    "source_loader = torch.utils.data.DataLoader(dataset[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "target_loader = torch.utils.data.DataLoader(dataset[\"val\"], batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset[\"test\"], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical, kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop \n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for i, (source, target) in enumerate(zip(source_loader, target_loader)):\n",
    "        optimizer.zero_grad()\n",
    "        # compute source and target logits\n",
    "        source_logits = heads(source[\"image\"]) # batch, n_heads\n",
    "        target_logits = heads(target[\"image\"]) # batch, n_heads\n",
    "        # compute source loss (cross entropy)\n",
    "        source_loss = loss_func(source_logits, source[\"y\"])\n",
    "        # compute divdis loss\n",
    "        divdis_loss = divdis_loss_func(target_logits)\n",
    "        # compute regularization loss\n",
    "        reg_loss = reg_loss_func(source_logits, target_logits)\n",
    "        loss = source_loss + lambda_1 * divdis_loss + lambda_2 * reg_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what framework am I going to use?\n",
    "\n",
    "# initialize number of heads \n",
    "# initialize optimization or whatever \n",
    "# divide train set into train and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to define mi loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok, now maybe I'll just use the huggingface trainer?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
