{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, lets get the imports we need \n",
    "from typing import List, Tuple, Dict, Any, Set\n",
    "from collections import defaultdict\n",
    "from itertools import product, chain\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from auto_circuit.data import load_datasets_from_json\n",
    "from auto_circuit.experiment_utils import load_tl_model\n",
    "from auto_circuit.prune_algos.mask_gradient import mask_gradient_prune_scores\n",
    "from auto_circuit.prune_algos.edge_attribution_patching import edge_attribution_patching_prune_scores\n",
    "from auto_circuit.data import BatchKey, PromptDataLoader\n",
    "from auto_circuit.types import AblationType, PatchType, PruneScores, CircuitOutputs\n",
    "from auto_circuit.utils.ablation_activations import src_ablations, batch_src_ablations\n",
    "from auto_circuit.utils.graph_utils import patch_mode, patchable_model, set_all_masks, train_mask_mode\n",
    "from auto_circuit.utils.tensor_ops import batch_avg_answer_diff\n",
    "from auto_circuit.utils.misc import repo_path_to_abs_path\n",
    "from auto_circuit.visualize import draw_seq_graph\n",
    "from auto_circuit.types import Edge, Node\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oliverdaniels-koch/Library/Caches/pypoetry/virtualenvs/elk-experiments-AZ2LBS3Q-py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\" #TODO: debug mps error\n",
    "model = load_tl_model(\"gpt2-small\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorized=True\n",
    "slice_output=\"last_seq\"\n",
    "separate_qkv=True\n",
    "seq_len=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_circuit.model_utils.transformer_lens_utils import factorized_dest_nodes, factorized_src_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = patchable_model(\n",
    "    model, \n",
    "    factorized=factorized, \n",
    "    slice_output=slice_output, \n",
    "    separate_qkv=separate_qkv, \n",
    "    seq_len=seq_len,\n",
    "    resid_src=False, \n",
    "    resid_dest=False,\n",
    "    attn_src=True,\n",
    "    attn_dest=True,\n",
    "    mlp_src=True,\n",
    "    mlp_dest=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = repo_path_to_abs_path(\"datasets/ioi/ioi_vanilla_template_prompts.json\")\n",
    "train_loader, test_loader = load_datasets_from_json(\n",
    "    model=model,\n",
    "    path=path,\n",
    "    device=device,\n",
    "    prepend_bos=True,\n",
    "    batch_size=16,\n",
    "    train_test_size=(128, 128),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bce9f44319c43ee80bd14611bfb7943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(          | 0/1 [00:00<?, ?it/s],))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attribution_scores: PruneScores = mask_gradient_prune_scores(\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    official_edges=None,\n",
    "    grad_function=\"logit\",\n",
    "    answer_function=\"avg_diff\",\n",
    "    mask_val=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elk-experiments-AZ2LBS3Q-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
